{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "#VSC-85fe2333",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "## CSC-696-001.2025F Final Project(2/3)",
                "**Name: Anna Hyunjung Kim**",
                "",
                "**Collaborators: Prof. Patrick Wu**",
                "",
                "",
                "",
                "",
                "",
                "---",
                "",
                "",
                "",
                "",
                "",
                "**Title:** Measuring Ethical Risks in AI-Generated News Using NLP with the UNESCO Ethics of AI Framework",
                "",
                "**Research Question:** How many problematic errors occur ethically in news articles generated by AI to some extent. Also, which category of the AI ethics principles proposed by UNESCO do these issues correspond closest to?"
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-631c9674",
            "metadata": {
                "language": "python"
            },
            "source": [
                "import json",
                "import requests",
                "from collections import Counter",
                "import pandas as pd",
                "from datasets import Dataset, DatasetDict",
                "",
                "BASE_URL = \"https://raw.githubusercontent.com/punyajoy/HateXplain/master/Data/\"",
                "",
                "dataset_json = requests.get(BASE_URL + \"dataset.json\").json()",
                "split_ids    = requests.get(BASE_URL + \"post_id_divisions.json\").json()",
                ""
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-c7181f1d",
            "metadata": {
                "language": "python"
            },
            "source": [
                "",
                "id2label_str = {0: \"hatespeech\", 1: \"normal\", 2: \"offensive\"}",
                "label_str2id = {\"hatespeech\": 0, \"normal\": 1, \"offensive\": 2}",
                "",
                "def normalize_label(lab):",
                "    if isinstance(lab, int):",
                "        return id2label_str[lab]",
                "    return lab",
                "",
                "def build_split(split_key):",
                "    rows = []",
                "    for pid in split_ids[split_key]:",
                "        info = dataset_json[pid]",
                "        tokens = info[\"post_tokens\"]",
                "        text   = \" \".join(tokens)",
                "",
                "",
                "        raw_labels = [ann[\"label\"] for ann in info[\"annotators\"]]",
                "        labels_norm = [normalize_label(l) for l in raw_labels]",
                "        maj_label_str = Counter(labels_norm).most_common(1)[0][0]",
                "        maj_label_id  = label_str2id[maj_label_str]",
                "",
                "        rows.append({",
                "            \"id\": pid,",
                "            \"text\": text,",
                "            \"label\": maj_label_id,",
                "        })",
                "",
                "    df = pd.DataFrame(rows)",
                "    return Dataset.from_pandas(df, preserve_index=False)",
                "",
                "train_ds = build_split(\"train\")",
                "val_ds   = build_split(\"val\")",
                "test_ds  = build_split(\"test\")",
                "",
                "dataset = DatasetDict({",
                "    \"train\": train_ds,",
                "    \"validation\": val_ds,",
                "    \"test\": test_ds,",
                "})",
                "",
                "print(dataset)",
                "print(dataset[\"train\"][0])",
                "# 0: \"hatespeech\", 1: \"normal\", 2: \"offensive\""
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-e4ba4ec5",
            "metadata": {
                "language": "python"
            },
            "source": [
                "#!pip install -q evaluate"
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-f88be6ac",
            "metadata": {
                "language": "python"
            },
            "source": [
                "from datasets import DatasetDict",
                "from transformers import (",
                "    AutoTokenizer,",
                "    AutoModelForSequenceClassification,",
                "    TrainingArguments,",
                "    Trainer,",
                ")",
                "import evaluate",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-2fbdd737",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# 0: \"hatespeech\", 1: \"normal\", 2: \"offensive\"",
                "id2label = {0: \"hatespeech\", 1: \"normal\", 2: \"offensive\"}",
                "label2id = {v: k for k, v in id2label.items()}",
                "print(id2label)",
                ""
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-3019536c",
            "metadata": {
                "language": "python"
            },
            "source": [
                "model_name = \"distilbert-base-uncased\"",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)",
                "",
                "def tokenize_batch(batch):",
                "    return tokenizer(",
                "        batch[\"text\"],",
                "        padding=\"max_length\",",
                "        truncation=True,",
                "        max_length=256,",
                "    )",
                "",
                "tokenized_dataset = dataset.map(tokenize_batch, batched=True)",
                "",
                "cols_to_remove = [c for c in tokenized_dataset[\"train\"].column_names",
                "                  if c not in [\"input_ids\", \"attention_mask\", \"label\"]]",
                "",
                "tokenized_dataset = tokenized_dataset.remove_columns(cols_to_remove)",
                "",
                "tokenized_dataset.set_format(\"torch\")",
                "",
                "tokenized_dataset",
                ""
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-8174a83f",
            "metadata": {
                "language": "python"
            },
            "source": [
                "model = AutoModelForSequenceClassification.from_pretrained(",
                "    model_name,",
                "    num_labels=3,",
                "    id2label=id2label,",
                "    label2id=label2id,",
                ")",
                ""
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-2abede42",
            "metadata": {
                "language": "python"
            },
            "source": [
                "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score",
                "",
                "def compute_metrics(eval_pred):",
                "    logits, labels = eval_pred",
                "    preds = np.argmax(logits, axis=-1)",
                "",
                "    acc  = accuracy_score(labels, preds)",
                "    f1   = f1_score(labels, preds, average=\"macro\")",
                "    prec = precision_score(labels, preds, average=\"macro\")",
                "    rec  = recall_score(labels, preds, average=\"macro\")",
                "",
                "    return {",
                "        \"accuracy\": acc,",
                "        \"f1_macro\": f1,",
                "        \"precision_macro\": prec,",
                "        \"recall_macro\": rec,",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-d4f648ba",
            "metadata": {
                "language": "python"
            },
            "source": [
                "training_args = TrainingArguments(",
                "    output_dir=\"./hatexplain_distilbert\",",
                "    num_train_epochs=3,",
                "    per_device_train_batch_size=16,",
                "    per_device_eval_batch_size=32,",
                "    learning_rate=2e-5,",
                "    weight_decay=0.01,",
                "    report_to=\"none\",",
                "    label_smoothing_factor=0.1",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-65a019ba",
            "metadata": {
                "language": "python"
            },
            "source": [
                "trainer = Trainer(",
                "    model=model,",
                "    args=training_args,",
                "    train_dataset=tokenized_dataset[\"train\"],",
                "    eval_dataset=tokenized_dataset[\"validation\"],",
                "    tokenizer=tokenizer,",
                "    compute_metrics=compute_metrics,",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-6c4d60ef",
            "metadata": {
                "language": "python"
            },
            "source": [
                "trainer.train()",
                "",
                "# validation",
                "eval_results = trainer.evaluate()",
                "print(eval_results)",
                "",
                "# test",
                "test_results = trainer.evaluate(tokenized_dataset[\"test\"])",
                "print(test_results)",
                ""
            ]
        },
        {
            "cell_type": "code",
            "id": "#VSC-a2d88703",
            "metadata": {
                "language": "python"
            },
            "source": [
                "from google.colab import drive",
                "import os",
                "drive.mount('/content/drive')",
                "",
                "save_dir = \"/content/drive/MyDrive/models/hatexplain_distilbert\"",
                "",
                "os.makedirs(save_dir, exist_ok=True)",
                "",
                "trainer.save_model(save_dir)",
                "tokenizer.save_pretrained(save_dir)"
            ]
        }
    ]
}
